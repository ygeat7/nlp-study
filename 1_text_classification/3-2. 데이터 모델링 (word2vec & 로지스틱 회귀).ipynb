{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_IN_PATH = './data_in/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "\n",
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
    "\n",
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])\n",
    "\n",
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec 벡터화\n",
    "# 하이퍼파라미터 정의\n",
    "num_features = 300    # 워드 벡터 특징값 수 -> 한개의 단어가 해당 특징 값 수로 표현된 벡터로 구성\n",
    "min_word_count = 40   # 단어에 대한 최소 빈도 수 -> 해당 값 이하로 사용되는 단어는 배제\n",
    "num_workers = 4       # 프로세스 개수\n",
    "context = 10          # 컨텍스트 윈도 크기\n",
    "downsampling = 1e-3   # 다운 샘플링 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0.tar.gz (23.2 MB)\n",
      "     |████████████████████████████████| 23.2 MB 142 kB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
      "Collecting smart_open>=1.8.1\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "     |████████████████████████████████| 57 kB 4.6 MB/s             \n",
      "\u001b[?25hBuilding wheels for collected packages: gensim\n",
      "  Building wheel for gensim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gensim: filename=gensim-4.2.0-cp36-cp36m-linux_x86_64.whl size=25519652 sha256=cea6ef687dda7829d1fb420f1d4661a2b87c8a2f6be8cb8778e05a90308d5e3b\n",
      "  Stored in directory: /root/.cache/pip/wheels/44/1e/2b/b0056a533d057c3ed56c84fbdd79cca690496f4cd7c03c157c\n",
      "Successfully built gensim\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.2.0 smart-open-6.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: dataclasses\n",
      "Successfully installed dataclasses-0.8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 07:27:36,094 : INFO : collecting all words and their counts\n",
      "2023-10-06 07:27:36,095 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 07:27:36,399 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2023-10-06 07:27:36,673 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2023-10-06 07:27:36,809 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2023-10-06 07:27:36,810 : INFO : Creating a fresh vocabulary\n",
      "2023-10-06 07:27:36,918 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 8160 unique words (11.02% of original 74065, drops 65905)', 'datetime': '2023-10-06T07:27:36.917412', 'gensim': '4.2.0', 'python': '3.6.9 (default, Nov  7 2019, 10:44:02) \\n[GCC 8.3.0]', 'platform': 'Linux-5.15.0-84-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2023-10-06 07:27:36,918 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 2627273 word corpus (87.92% of original 2988089, drops 360816)', 'datetime': '2023-10-06T07:27:36.918800', 'gensim': '4.2.0', 'python': '3.6.9 (default, Nov  7 2019, 10:44:02) \\n[GCC 8.3.0]', 'platform': 'Linux-5.15.0-84-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2023-10-06 07:27:37,020 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2023-10-06 07:27:37,022 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2023-10-06 07:27:37,023 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2494384.49928802 word corpus (94.9%% of prior 2627273)', 'datetime': '2023-10-06T07:27:37.023300', 'gensim': '4.2.0', 'python': '3.6.9 (default, Nov  7 2019, 10:44:02) \\n[GCC 8.3.0]', 'platform': 'Linux-5.15.0-84-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2023-10-06 07:27:37,168 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2023-10-06 07:27:37,169 : INFO : resetting layer weights\n",
      "2023-10-06 07:27:37,186 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-10-06T07:27:37.186700', 'gensim': '4.2.0', 'python': '3.6.9 (default, Nov  7 2019, 10:44:02) \\n[GCC 8.3.0]', 'platform': 'Linux-5.15.0-84-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'build_vocab'}\n",
      "2023-10-06 07:27:37,187 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2023-10-06T07:27:37.187474', 'gensim': '4.2.0', 'python': '3.6.9 (default, Nov  7 2019, 10:44:02) \\n[GCC 8.3.0]', 'platform': 'Linux-5.15.0-84-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n",
      "2023-10-06 07:27:38,198 : INFO : EPOCH 0 - PROGRESS: at 37.01% examples, 928930 words/s, in_qsize 7, out_qsize 0\n",
      "2023-10-06 07:27:39,201 : INFO : EPOCH 0 - PROGRESS: at 76.69% examples, 955990 words/s, in_qsize 7, out_qsize 0\n",
      "2023-10-06 07:27:39,785 : INFO : EPOCH 0: training on 2988089 raw words (2494395 effective words) took 2.6s, 961803 effective words/s\n",
      "2023-10-06 07:27:40,799 : INFO : EPOCH 1 - PROGRESS: at 39.12% examples, 973474 words/s, in_qsize 7, out_qsize 0\n",
      "2023-10-06 07:27:41,800 : INFO : EPOCH 1 - PROGRESS: at 78.01% examples, 971108 words/s, in_qsize 7, out_qsize 0\n",
      "2023-10-06 07:27:42,341 : INFO : EPOCH 1: training on 2988089 raw words (2494273 effective words) took 2.6s, 977177 effective words/s\n",
      "2023-10-06 07:27:43,351 : INFO : EPOCH 2 - PROGRESS: at 34.78% examples, 870497 words/s, in_qsize 7, out_qsize 0\n",
      "2023-10-06 07:27:44,351 : INFO : EPOCH 2 - PROGRESS: at 69.63% examples, 870479 words/s, in_qsize 7, out_qsize 0\n",
      "2023-10-06 07:27:45,147 : INFO : EPOCH 2: training on 2988089 raw words (2493912 effective words) took 2.8s, 889812 effective words/s\n",
      "2023-10-06 07:27:46,152 : INFO : EPOCH 3 - PROGRESS: at 37.39% examples, 941013 words/s, in_qsize 7, out_qsize 0\n",
      "2023-10-06 07:27:47,157 : INFO : EPOCH 3 - PROGRESS: at 76.69% examples, 957666 words/s, in_qsize 7, out_qsize 0\n",
      "2023-10-06 07:27:47,736 : INFO : EPOCH 3: training on 2988089 raw words (2494595 effective words) took 2.6s, 964812 effective words/s\n",
      "2023-10-06 07:27:48,754 : INFO : EPOCH 4 - PROGRESS: at 38.77% examples, 962812 words/s, in_qsize 7, out_qsize 0\n",
      "2023-10-06 07:27:49,772 : INFO : EPOCH 4 - PROGRESS: at 79.41% examples, 978126 words/s, in_qsize 8, out_qsize 0\n",
      "2023-10-06 07:27:50,275 : INFO : EPOCH 4: training on 2988089 raw words (2494317 effective words) took 2.5s, 983944 effective words/s\n",
      "2023-10-06 07:27:50,276 : INFO : Word2Vec lifecycle event {'msg': 'training on 14940445 raw words (12471492 effective words) took 13.1s, 952881 effective words/s', 'datetime': '2023-10-06T07:27:50.276279', 'gensim': '4.2.0', 'python': '3.6.9 (default, Nov  7 2019, 10:44:02) \\n[GCC 8.3.0]', 'platform': 'Linux-5.15.0-84-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n",
      "2023-10-06 07:27:50,276 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=8160, vector_size=300, alpha=0.025>', 'datetime': '2023-10-06T07:27:50.276889', 'gensim': '4.2.0', 'python': '3.6.9 (default, Nov  7 2019, 10:44:02) \\n[GCC 8.3.0]', 'platform': 'Linux-5.15.0-84-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "\n",
    "# 기본적으로 CBOW 방식으로 학습 (초기화 매개변수 sg=0 / sg=1일 경우 skip-gram 방식으로 학습)\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                          workers = num_workers,\n",
    "                          vector_size = num_features,\n",
    "                          min_count = min_word_count,\n",
    "                          window = context,\n",
    "                          sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 10:20:14,722 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-10-05T10:20:14.722685', 'gensim': '4.2.0', 'python': '3.6.9 (default, Nov  7 2019, 10:44:02) \\n[GCC 8.3.0]', 'platform': 'Linux-5.15.0-84-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'saving'}\n",
      "2023-10-05 10:20:14,724 : INFO : not storing attribute cum_table\n",
      "2023-10-05 10:20:14,790 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words, model, num_features):\n",
    "    # 출력 벡터 초기화\n",
    "    feature_vector = np.zeros((num_features), dtype=np.float32)\n",
    "    \n",
    "    num_words = 0\n",
    "    # 어휘 사전 준비\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            # 사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "            feature_vector = np.add(feature_vector, model.wv[w])\n",
    "    # 문장의 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로 함\n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector\n",
    "# 단어 1개가 (300,)의 값으로 구성된 np배열\n",
    "# 모든 문장의 벡터 값을 더하고, 단어의 개수만큼 나누어 평균을 냄\n",
    "# 이렇게 하여 문장 1개를 (300,)의 값으로 구성된 np 배열로 출력 (단순한 방법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "    \n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "        \n",
    "    reviewFeatureVecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeatureVecs\n",
    "# reviews : 학습 데이터인 전체 리뷰 데이터를 입력하는 인자\n",
    "# model : word2vec 모델을 입력하는 인자\n",
    "# num_featrues : word2vec 모델 임베딩 차원 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(len(test_data_vecs))\n",
    "print(test_data_vecs[0].shape) # 1개의 문장이 (300,)의 1차원 벡터로 구성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "X = test_data_vecs\n",
    "y = np.array(sentiments)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습 - 로지스틱 회귀\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.866000\n"
     ]
    }
   ],
   "source": [
    "# 검증데이터로 성능 측정\n",
    "print(\"Accuracy: %f\" % lgs.score(X_eval, y_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 캐글 데이터 제출\n",
    "TEST_CLEAN_DATA= 'test_clean.csv'\n",
    "\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "\n",
    "test_review = list(test_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naturally film main themes mortality nostalgia loss innocence perhaps surprising rated highly older viewers younger ones however craftsmanship completeness film anyone enjoy pace steady constant characters full engaging relationships interactions natural showing need floods tears show emotion screams show fear shouting show dispute violence show anger naturally joyce short story lends film ready made structure perfect polished diamond small changes huston makes inclusion poem fit neatly truly masterpiece tact subtlety overwhelming beauty'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 문자열을 단어 리스트로 변환\n",
    "test_sentences = []\n",
    "for review in test_review:\n",
    "    test_sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "test_data_vecs = get_dataset(test_sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './data_out/'\n",
    "\n",
    "test_predicted = lgs.predict(test_data_vecs)\n",
    "\n",
    "ids = list(test_data['id'])\n",
    "answer_dataset = pd.DataFrame({'id':ids, 'sentiment':test_predicted})\n",
    "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_w2v_answer.csv', index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
